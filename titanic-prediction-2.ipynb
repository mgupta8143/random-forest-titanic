{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-17T06:24:53.928160Z","iopub.execute_input":"2022-02-17T06:24:53.928438Z","iopub.status.idle":"2022-02-17T06:24:53.939374Z","shell.execute_reply.started":"2022-02-17T06:24:53.928410Z","shell.execute_reply":"2022-02-17T06:24:53.937937Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Note this is a pure classification task. There are many different options out there, as listed below:\n* Neural Networks (Cross-Entropy would work well here)\n* Clustering (KMeans, DBSCAN, Hierarchal, GMM, etc.)\n* Ensembles with SOTA\n* Etc, etc.\n\nFor this notebook, because I'm currently learning more about basic ML, I will try using a decision tree and evaluate performance. \n\nFirst, I'm going to analyze the data to see if there are any issues with it","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/titanic/train.csv\")\n\ndisplay(df.describe())\ndisplay(df.head())\nprint(df.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:53.984674Z","iopub.execute_input":"2022-02-17T06:24:53.985775Z","iopub.status.idle":"2022-02-17T06:24:54.036186Z","shell.execute_reply.started":"2022-02-17T06:24:53.985715Z","shell.execute_reply":"2022-02-17T06:24:54.035556Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"num_vars = df.columns[df.dtypes != 'object']\ncat_vars = df.columns[df.dtypes == 'object']\n\nprint(num_vars)\nprint(cat_vars)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:54.037864Z","iopub.execute_input":"2022-02-17T06:24:54.038283Z","iopub.status.idle":"2022-02-17T06:24:54.045919Z","shell.execute_reply.started":"2022-02-17T06:24:54.038245Z","shell.execute_reply":"2022-02-17T06:24:54.045026Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"df[num_vars].isnull().sum().sort_values(ascending=False)/len(df)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:54.047439Z","iopub.execute_input":"2022-02-17T06:24:54.047727Z","iopub.status.idle":"2022-02-17T06:24:54.067035Z","shell.execute_reply.started":"2022-02-17T06:24:54.047702Z","shell.execute_reply":"2022-02-17T06:24:54.066250Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df[cat_vars].isnull().sum().sort_values(ascending=False)/len(df)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:54.069078Z","iopub.execute_input":"2022-02-17T06:24:54.070271Z","iopub.status.idle":"2022-02-17T06:24:54.087157Z","shell.execute_reply.started":"2022-02-17T06:24:54.070212Z","shell.execute_reply":"2022-02-17T06:24:54.086626Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Evidently, we can see that most of the data is present, however for the cabin, column, a lot of data is not there, hence we will remove it. For age, we will not as 80% of the rows will, and for now we will settle with filling the remaining 20% with the mean of the existing values. For embarked, we will just fill the remaining with the most column letter.","metadata":{}},{"cell_type":"code","source":"df[num_vars] = df[num_vars].apply(lambda col: col.fillna(col.mean()))\ndf = df.drop(columns=['Cabin', 'PassengerId'])\nnum_vars = df.columns[df.dtypes != 'object']\ncat_vars = df.columns[df.dtypes == 'object']\ndf[cat_vars] = df[cat_vars].apply(lambda col: col.fillna(col.mode()[0]))\n\ndisplay(df[num_vars].isnull().sum().sort_values(ascending=False)/len(df))\ndisplay(df[cat_vars].isnull().sum().sort_values(ascending=False)/len(df))\n\nsex = pd.get_dummies(df['Sex'], drop_first=True)\nembark = pd.get_dummies(df['Embarked'], drop_first=True)\npclass = pd.get_dummies(df['Pclass'], drop_first=True)\n\n\ndf = pd.concat([df, sex, embark, pclass], axis=1)\ndf.drop(['Sex', 'Embarked', 'Pclass', \"Ticket\", \"Name\"], axis=1, inplace=True)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:54.088057Z","iopub.execute_input":"2022-02-17T06:24:54.088988Z","iopub.status.idle":"2022-02-17T06:24:54.140960Z","shell.execute_reply.started":"2022-02-17T06:24:54.088925Z","shell.execute_reply":"2022-02-17T06:24:54.139565Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Now that the data has been cleaned, we can start using our decision tree classifier.","metadata":{}},{"cell_type":"code","source":"dataset = df.to_numpy()\nprint(dataset.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:54.142150Z","iopub.execute_input":"2022-02-17T06:24:54.142451Z","iopub.status.idle":"2022-02-17T06:24:54.148847Z","shell.execute_reply.started":"2022-02-17T06:24:54.142415Z","shell.execute_reply":"2022-02-17T06:24:54.147242Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"X = dataset[:, 1:]\nY = dataset[:, 0]","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:54.150187Z","iopub.execute_input":"2022-02-17T06:24:54.150811Z","iopub.status.idle":"2022-02-17T06:24:54.164742Z","shell.execute_reply.started":"2022-02-17T06:24:54.150780Z","shell.execute_reply":"2022-02-17T06:24:54.164042Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nfrom matplotlib import pyplot as plt\n\nscaler = StandardScaler()\nVARIANCE = 0.98\npca = PCA(VARIANCE)\n\nX = scaler.fit_transform(X)\nX = pca.fit_transform(X)\n\n#distorsions = []\n#for k in range(2, 30):\n #   kmeans = KMeans(init='k-means++', n_init = 10, n_clusters = k).fit(X)\n  #  distorsions.append(kmeans.inertia_)\n\nfig = plt.figure(figsize=(15, 5))\nplt.plot(range(2, 30), distorsions)\nplt.grid(True)\nplt.title('Elbow curve')\ndisplay(plt.show())\n\nkmeans = KMeans(init='k-means++', n_init = 10, n_clusters = 25).fit(X)\n\n\ndic = {}\nfor i in range(len(kmeans.labels_)):\n    val = kmeans.labels_[i]\n    if val in dic:\n        dic[val][int(Y[i])] += 1\n    else:\n        dic[val] = [0 for i in range(26)]\n        dic[val][int(Y[i])] += 1\n\nfor key in dic:\n    dic[key] = np.argmax(dic[key])\n\nprint(dic)    \n\n\n\ntest_df = pd.read_csv(\"../input/titanic/test.csv\")\ntest_df = test_df.drop(columns=['Cabin', 'PassengerId'])\nnum_vars = test_df.columns[test_df.dtypes != 'object']\ncat_vars = test_df.columns[test_df.dtypes == 'object']\ntest_df[num_vars] = test_df[num_vars].apply(lambda col: col.fillna(col.mean()))\ndisplay(test_df[num_vars].isnull().sum().sort_values(ascending=False)/len(df))\ndisplay(test_df[cat_vars].isnull().sum().sort_values(ascending=False)/len(df))\n\nsex = pd.get_dummies(test_df['Sex'], drop_first=True)\nembark = pd.get_dummies(test_df['Embarked'], drop_first=True)\npclass = pd.get_dummies(test_df['Pclass'], drop_first=True)\n\ntest_df = pd.concat([test_df, sex, embark, pclass], axis=1)\ntest_df.drop(['Sex', 'Embarked', 'Pclass', \"Ticket\", \"Name\"], axis=1, inplace=True)\ndisplay(test_df.head())\n\n\ntest_X = test_df.to_numpy()\ntest_X = scaler.fit_transform(test_X)\ntest_X = pca.transform(test_X)\n\npredictions_int = kmeans.predict(test_X)\n\npredictions = []\nfor i in range(len(predictions_int)):\n    if predictions_int[i] in dic:\n        predictions.append(dic[predictions_int[i]])\n    else:\n        predictions.append(0)\n\nprint(predictions)\n        \ninter = []\ncount = 892\nfor i in range(len(predictions)):\n    inter.append([count, int(predictions[i])])\n    count += 1\n\noutput = pd.DataFrame(inter, columns=[\"PassengerId\", \"Survived\"])\noutput.to_csv('predictions_fixed.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-17T06:24:54.165723Z","iopub.execute_input":"2022-02-17T06:24:54.166000Z","iopub.status.idle":"2022-02-17T06:24:58.362721Z","shell.execute_reply.started":"2022-02-17T06:24:54.165968Z","shell.execute_reply":"2022-02-17T06:24:58.362112Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}